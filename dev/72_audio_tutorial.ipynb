{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.basics import *\n",
    "from local.data.all import *\n",
    "from local.vision.core import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.audio.core import *\n",
    "from local.audio.augment import *\n",
    "from local.vision.learner import *\n",
    "from local.vision.models.xresnet import *\n",
    "from local.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Training a Voice Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/h/.fastai/data/ST-AEDS-20180100_1-OS/ST-AEDS-20180100_1-OS')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p10speakers = Config()['data_path'] / 'ST-AEDS-20180100_1-OS'\n",
    "untar_data(URLs.SPEAKERS10, dest=p10speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4452253696' class='' max='9220510576', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      48.29% [4452253696/9220510576 06:44<07:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Warning this dataset is ~8GB\n",
    "p250speakers = Config()['data_path'] / '250_speakers'\n",
    "untar_data(URLs.SPEAKERS250, fname=str(p250speakers)+'.tar', dest=p250speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = AudioGetter(\"\", recurse=True, folders=None)\n",
    "files_10  = x(p10speakers)\n",
    "files_250 = x(p250speakers)\n",
    "#original_aud = AudioItem.create(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datablock and Basic End to End Training on 10 Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AudioBlock(cls=AudioItem): return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_items=get_audio_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=lambda x: str(x).split('/')[-1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [y for _,y in auds.datasource(p10speakers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify categories are being correctly assigned\n",
    "test_eq(min(cats).item(), 0)\n",
    "test_eq(max(cats).item(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop 2s from the signal and turn it to a MelSpectrogram with no augmentation\n",
    "cfg_voice = AudioConfig.Voice()\n",
    "a2s = AudioToSpec.from_cfg(cfg_voice)\n",
    "crop_2000ms = CropSignal(2000)\n",
    "tfms = Pipeline([crop_2000ms, a2s], as_item=True)\n",
    "dbunch = auds.databunch(p10speakers, item_tfms=tfms, bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-warning\"><strong>Broken:</strong><br>Show batch is broken as it appears to just be grabbing the data from the sg, and not the sg object itself, but calls the sg's show method which relies on nchannels, which is an object of AudioSpectrogram (part of sg settings but we overrode getattr to make it work like an attribute). This means the items cant show themselves for the batch, but training still works </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbunch_cropspec.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to Kevin Bird and Hiromi Suenaga for these two lines to adjust a CNN model to take 1 channel input\n",
    "def alter_learner(learn, channels=1):\n",
    "    learn.model[0][0].in_channels=channels\n",
    "    learn.model[0][0].weight = torch.nn.parameter.Parameter(learn.model[0][0].weight[:,1,:,:].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "nchannels = dbunch.one_batch()[0].shape[1]\n",
    "alter_learner(learn, nchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs are a bit longer due to the chosen melspectrogram settings\n",
    "learn.fit_one_cycle(10, lr_max=slice(1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on 250 Speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(random.choice(files_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lens = stats(files_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_250speakers_label = lambda x: str(x).split('/')[-3][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    f = random.choice(files_250)\n",
    "    print(\"File:\",f )\n",
    "    print(\"Label:\", get_250speakers_label(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_items=get_audio_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=get_250speakers_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch250 = auds.databunch(p250speakers, item_tfms=tfms, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [y for _,y in auds.datasource(p250speakers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify categories are being correctly assigned for 250 speakers\n",
    "test_eq(min(cats).item(), 0)\n",
    "test_eq(max(cats).item(), 249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torchaudio default MelSpectrogram to get a baseline\n",
    "a2s = AudioToSpec()\n",
    "crop_4000ms = CropSignal(4000)\n",
    "tfms = Pipeline([crop_4000ms, a2s], as_item=True)\n",
    "dbunch = auds.databunch(p250speakers, item_tfms=tfms, bs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "nchannels = dbunch.one_batch()[0].shape[1]\n",
    "alter_learner(learn, nchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, lr_max=slice(2e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5, lr_max=slice(1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize our AudioToSpec Function using a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_cfg = AudioConfig.Voice()\n",
    "a2s = AudioToSpec.from_cfg(voice_cfg)\n",
    "tfms = Pipeline([crop_4000ms, a2s], as_item=True)\n",
    "# tfms = Pipeline([CropSignal(4000),  a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True)\n",
    "dbunch = auds.databunch(p250speakers, item_tfms=tfms, bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "nchannels = dbunch.one_batch()[0].shape[1]\n",
    "alter_learner(learn, nchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better results even without fine tuning, but much slower. We need to move a2s to the GPU and \n",
    "# then add data augmentation!\n",
    "learn.fit_one_cycle(5, lr_max=slice(2e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an MFCC with Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only grab 1500ms of the clip, voice identity can be done with shorter sections and it will speed it up\n",
    "# this is really slow for mfcc, even for 45k files, need to figure out what's going on here\n",
    "a2mfcc = AudioToMFCC(n_mffc=20, melkwargs={\"n_fft\":2048, \"hop_length\":256, \"n_mels\":128})\n",
    "tfms = Pipeline([CropSignal(1500), a2mfcc, Delta()], as_item=True)\n",
    "# tfms = Pipeline([CropSignal(4000),  a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True)\n",
    "dbunch = auds.databunch(p250speakers, item_tfms=tfms, bs=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lr_max=slice(2e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(7, lr_max=slice(3e-3, 4e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'><strong>From Here:</strong><br>\n",
    "    1. Get transforms on the GPU <br>\n",
    "    2. Once it's faster test signal and spectrogram augments for speed/efficacy<br>\n",
    "    3. Fine-tune and see how high we can push results on 250 speakers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imperfect GPU Spectrogram Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskFreqBatch(num_masks=1, size=20, start=None, val=None, **kwargs):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        '''Google SpecAugment time masking from https://arxiv.org/abs/1904.08779.'''\n",
    "        start_ = start\n",
    "        sg = spectro.clone()\n",
    "        channel_mean = sg.contiguous().view(sg.size(0), -1).mean(-1)[:,None,None]\n",
    "        mask_val = channel_mean if val is None else val\n",
    "        c, y, x = sg.shape[-3:]\n",
    "        for _ in range(num_masks):\n",
    "            #print(\"Mask Val\", mask_val)\n",
    "            #Currently not worrying about getting the channel mean from batchwise implementation\n",
    "            mask = torch.ones(size, x, device=device).unsqueeze(0).unsqueeze(0) #* mask_val    \n",
    "            if start_ is None: start_= random.randint(0, y-size)\n",
    "            if not 0 <= start_ <= y-size:\n",
    "                raise ValueError(f\"Start value '{start_}' out of range for AudioSpectrogram of shape {sg.shape}\")\n",
    "            sg[...,start_:start_+size,:] = mask\n",
    "            start_ = None\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('/home/jupyter/.fastai/data/ST-AEDS-20180100_1-OS')\n",
    "cropsig_2000ms = CropSignal(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGRollBatch(max_shift_pct=0.5, direction=0, **kwargs):\n",
    "    '''Shifts spectrogram along x-axis wrapping around to other side'''\n",
    "    if int(direction) not in [-1, 0, 1]: \n",
    "        raise ValueError(\"Direction must be -1(left) 0(bidirectional) or 1(right)\")\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        nonlocal direction\n",
    "        direction = random.choice([-1, 1]) if direction == 0 else direction\n",
    "        sg = spectro.clone()\n",
    "        c, height, width = sg.shape[-3:]\n",
    "        roll_by = int(width*random.random()*max_shift_pct*direction)\n",
    "        sg = sg.roll(roll_by, dims=-1)\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfms = Pipeline([cropsig_2000ms, a2s_baseline], as_item=True)\n",
    "dl_tfms = Pipeline([Cuda(), MaskFreqBatch(), SGRollBatch()], as_item=True)\n",
    "dbunch = auds.databunch(p, item_tfms=ds_tfms, batch_tfms=dl_tfms, bs=64)\n",
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch,\n",
    "                xresnet18(),  \n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "alter_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lr_max=2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Attempt with correct mean channel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskFreqBatchValue(num_masks=1, size=20, start=None, val=None, **kwargs):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        '''Google SpecAugment time masking from https://arxiv.org/abs/1904.08779.'''\n",
    "        start_ = start\n",
    "        sg = spectro.clone()\n",
    "        channel_mean = sg.contiguous().view(sg.size(0), -1).mean(-1)[:,None,None,None]\n",
    "        mask_val = channel_mean if val is None else val\n",
    "        b, c, y, x = sg.shape\n",
    "        for _ in range(num_masks):\n",
    "            ones_mask = torch.ones(size=(1, c, size, x), device=device)\n",
    "            mask = ones_mask * mask_val\n",
    "            if start_ is None: start_= random.randint(0, y-size)\n",
    "            if not 0 <= start_ <= y-size:\n",
    "                raise ValueError(f\"Start value '{start_}' out of range for AudioSpectrogram of shape {sg.shape}\")\n",
    "            sg[:,:,start_:start_+size,:] = mask\n",
    "            start_ = None\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.ones(size=(64,1,20,128)) * 12).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfms = Pipeline([cropsig_2000ms, a2s_baseline], as_item=True)\n",
    "dl_tfms = Pipeline([Cuda(), MaskFreqBatchValue()], as_item=True)\n",
    "dbunch = auds.databunch(p, item_tfms=ds_tfms, batch_tfms=dl_tfms, bs=64)\n",
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd Attempt with correct random placement of mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskFreqBatchPlace(num_masks=1, size=20, start=None, val=None, **kwargs):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        '''Google SpecAugment time masking from https://arxiv.org/abs/1904.08779.'''\n",
    "        start_ = start\n",
    "        sg = spectro.clone()\n",
    "        channel_mean = sg.contiguous().view(sg.size(0), -1).mean(-1)[:,None,None,None]\n",
    "        mask_val = channel_mean if val is None else val\n",
    "        b, c, y, x = sg.shape\n",
    "        for _ in range(num_masks):\n",
    "            ones_mask = torch.ones(size=(1, c, size, x), device=device)\n",
    "            mask = ones_mask * mask_val\n",
    "            if start_ is None: start_= random.randint(0, y-size)\n",
    "            if not 0 <= start_ <= y-size:\n",
    "                raise ValueError(f\"Start value '{start_}' out of range for AudioSpectrogram of shape {sg.shape}\")\n",
    "            start_test = np.arange(0,,1)\n",
    "            #print(\"Index: shape\", start_test.shape)\n",
    "            sg[:,:,start_:start_+size,:] = mask\n",
    "            start_ = None\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfms = Pipeline([cropsig_2000ms, a2s_baseline], as_item=True)\n",
    "dl_tfms = Pipeline([Cuda(), MaskFreqBatchPlace()], as_item=True)\n",
    "dbunch = auds.databunch(p, item_tfms=ds_tfms, batch_tfms=dl_tfms, bs=4)\n",
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.ones(size=(64,1,20,128)) * 12).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch = torch.stack([CropTime(2000)(a2s(AudioItem.create(files[i]))) for i in range(64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.array([2,4,6,8], dtype=np.intp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index + 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch[:,:,index:index+20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch[:,:,torch.arange(0, 7).long(),:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
