{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp audio.augment\n",
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation for Audio\n",
    "\n",
    "> Transforms to apply data augmentation to AudioSpectrograms and Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.data.all import *\n",
    "from local.vision.core import *\n",
    "from local.vision.augment import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.audio.core import *\n",
    "from local.learner import *\n",
    "from local.vision.models.xresnet import *\n",
    "from local.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch.nn\n",
    "from torch import stack, zeros_like as t0, ones_like as t1\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from librosa.effects import split\n",
    "from dataclasses import asdict\n",
    "from scipy.signal import resample_poly\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import librosa\n",
    "import colorednoise as cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Preprocessing Functions](#Preprocessing-Functions)  \n",
    "    1. [Remove Silence](#Remove-Silence)\n",
    "    1. [Resampling](#Resampling)\n",
    "1. [Signal Transforms](#Signal-Transforms)\n",
    "    1. [Signal Cropping/Padding](#Signal-Cropping/Padding)\n",
    "    1. [Signal Shifting](#Signal-Shifting)\n",
    "    1. [Add Noise to Signal](#Add-Noise-to-Signal)\n",
    "    1. [Adjust Volume](#Adjust-Volume)\n",
    "    1. [Signal Cutout](#Signal-Cutout)\n",
    "    1. [Signal Loss](#Signal-Loss)\n",
    "    1. [DownmixMono](#DownmixMono)\n",
    "1. [Spectrogram Transforms](#Spectrogram-Transforms)\n",
    "    1. [Time Cropping](#Time-Cropping)\n",
    "    1. [Time and Frequency Masking (SpecAugment)](#Time-and-Frequency-Masking-(SpecAugment))\n",
    "    1. [Spectrogram Rolling](#Spectrogram-Rolling)\n",
    "    1. [Delta/Accelerate](#Delta/Accelerate)\n",
    "1. [Pipelines](#Pipelines)\n",
    "    1. [Signal Pipelines](#Signal-Pipelines)\n",
    "    1. [Spectrogram Pipelines](#Spectrogram-Pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##export\n",
    "#_all_ = ['AudioGetter', 'get_audio_files', 'AudioItem', 'OpenAudio', 'AudioSpectrogram', 'AudioToSpec',\n",
    " #       'SpectrogramConfig', 'AudioConfig', 'audio_extensions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Config()['data_path'] / 'ST-AEDS-20180100_1-OS'\n",
    "untar_data(URLs.SPEAKERS10, fname=str(p)+'.tar', dest=p)\n",
    "x = AudioGetter(\"\", recurse=True, folders=None)\n",
    "files = x(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files will load differently on different machines so we specify examples by name\n",
    "ex_files = [p/f for f in ['m0005_us_m0005_00218.wav', \n",
    "                                'f0003_us_f0003_00279.wav', \n",
    "                                'f0001_us_f0001_00168.wav', \n",
    "                                'f0005_us_f0005_00286.wav',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_orig = AudioItem.create(ex_files[0])\n",
    "a2s = AudioToSpec(n_fft = 1024, hop_length=256)\n",
    "sg_orig = a2s(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 3 equal length portions of 3 different signals so we can stack them\n",
    "#for a fake multichannel example\n",
    "ai0, ai1, ai2 = map(AudioItem.create, ex_files[1:4]);\n",
    "min_samples = min(ai0.nsamples, ai1.nsamples, ai2.nsamples)\n",
    "s0, s1, s2 = map(lambda x: x[:,:min_samples], (ai0.sig, ai1.sig, ai2.sig))\n",
    "fake_multichannel = AudioItem((torch.stack((s0, s1, s2), dim=1).squeeze(0), 16000, None))\n",
    "sg_multi = a2s(fake_multichannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\"><strong>TO-DO:</strong><br/>\n",
    "    1. Add in longer clips (whale) and do more extensive testing. Current clip only allows us to test Trim, not All or Split<br/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Remove Silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mk_class('RemoveType', **{o:o.lower() for o in ['Trim', 'All', 'Split']},\n",
    "         doc=\"All methods of removing silence as attributes to get tab-completion and typo-proofing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _merge_splits(splits, pad):\n",
    "    clip_end = splits[-1][1]\n",
    "    merged = []\n",
    "    i=0\n",
    "    while i < len(splits):\n",
    "        start = splits[i][0]\n",
    "        while splits[i][1] < clip_end and splits[i][1] + pad >= splits[i+1][0] - pad:\n",
    "            i += 1\n",
    "        end = splits[i][1]\n",
    "        merged.append(np.array([max(start-pad, 0), min(end+pad, clip_end)]))\n",
    "        i+=1\n",
    "    return np.stack(merged)\n",
    "\n",
    "def RemoveSilence(remove_type=RemoveType.Trim, threshold=20, pad_ms=20):\n",
    "    def _inner(ai:AudioItem)->AudioItem:\n",
    "        '''Split signal at points of silence greater than 2*pad_ms '''\n",
    "        if remove_type is None: return ai\n",
    "        padding = int(pad_ms/1000*ai.sr)\n",
    "        if(padding > ai.nsamples): return ai\n",
    "        actual = ai.sig.clone()\n",
    "        splits = split(actual.numpy(), top_db=threshold, hop_length=padding)\n",
    "        if remove_type == \"split\":\n",
    "            sig =  [actual[:,(max(a-padding,0)):(min(b+padding,ai.nsamples))] \n",
    "                    for (a, b) in _merge_splits(splits, padding)]\n",
    "        elif remove_type == \"trim\":\n",
    "            sig = [actual[:,(max(splits[0, 0]-padding,0)):splits[-1, -1]+padding]]\n",
    "        elif remove_type == \"all\":\n",
    "            sig = [torch.cat([actual[:,(max(a-padding,0)):(min(b+padding,ai.nsamples))] \n",
    "                              for (a, b) in _merge_splits(splits, padding)], dim=1)]\n",
    "        else: \n",
    "            raise ValueError(f\"Valid options for silence removal are None, 'split', 'trim', 'all' not '{remove_type}'.\")\n",
    "        return AudioItem((*sig, ai.sr, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim Silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_audio = RemoveSilence(threshold=20, pad_ms=20)(audio_orig)\n",
    "audio_orig.show()\n",
    "silence_audio.show()\n",
    "#test that at least a half second of silence is being removed\n",
    "test(silence_audio.nsamples + 8000, audio_orig.nsamples, operator.le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test that nothing is removed from audio that doesnt contain silence\n",
    "test_aud = AudioItem((torch.rand_like(audio_orig.sig), 16000, None))\n",
    "print(\"Random Noise, no silence\")\n",
    "test_aud.hear()\n",
    "for rm_type in [RemoveType.All, RemoveType.Trim, RemoveType.Split]:\n",
    "    silence_audio_trim = RemoveSilence(rm_type, threshold=20, pad_ms=20)(test_aud)\n",
    "    test_eq(test_aud.nsamples, silence_audio_trim.nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim silence from a multichannel clip, needs more extensive testing\n",
    "silence_mc = RemoveSilence(threshold=20, pad_ms=20)(fake_multichannel)\n",
    "print(silence_mc.sig.shape) #still 3 channels\n",
    "fake_multichannel.hear()\n",
    "silence_mc.hear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim Silence Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silencer = RemoveSilence(threshold=20, pad_ms=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "silencer(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "silencer(fake_multichannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Resample(sr_new):\n",
    "    def _inner(ai:AudioItem)->AudioItem:\n",
    "        '''Resample using faster polyphase technique and avoiding FFT computation'''\n",
    "        if(ai.sr == sr_new): return AudioItem(ai)\n",
    "        sig_np = ai.sig.numpy()\n",
    "        sr_gcd = math.gcd(ai.sr, sr_new)\n",
    "        resampled = resample_poly(sig_np, int(sr_new/sr_gcd), int(ai.sr/sr_gcd), axis=-1)\n",
    "        resampled = resampled.astype(np.float32)\n",
    "        return AudioItem((torch.from_numpy(resampled), sr_new, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure if old and new sample rates are the same, a new identical AudioItem is returned\n",
    "no_resample_needed = Resample(audio_orig.sr)(audio_orig)\n",
    "assert(not no_resample_needed is audio_orig)\n",
    "test_eq(audio_orig.sr, no_resample_needed.sr)\n",
    "test_eq(audio_orig.sig, no_resample_needed.sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test and hear realistic sample rates\n",
    "print(\"Original, Sample Rate\", audio_orig.sr)\n",
    "audio_orig.hear()\n",
    "for rate in [4000,8000,22050,44100]:\n",
    "    resampled = Resample(rate)(audio_orig)\n",
    "    orig_samples = audio_orig.nsamples\n",
    "    re_samples = resampled.nsamples\n",
    "    print(\"Sample Rate\", rate)\n",
    "    resampled.hear()\n",
    "    test_eq(re_samples, orig_samples//(audio_orig.sr/rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample a multichannel audio\n",
    "resampled = Resample(8000)(fake_multichannel)\n",
    "test_eq(fake_multichannel.nsamples//2, resampled.nsamples)\n",
    "test_eq(fake_multichannel.nchannels, resampled.nchannels)\n",
    "test_eq(resampled.sr, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    random_sr = random.randint(16000, 72000)\n",
    "    random_upsample = Resample(random_sr)(audio_orig)\n",
    "    num_samples = random_upsample.nsamples\n",
    "    test_close(num_samples, abs(orig_samples//(audio_orig.sr/random_sr)), eps=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polyphase resampling's speed is dependent on the GCD between old and new rate. For almost all used sample rates it\n",
    "# will be very fast and much better than any FFT based method. It is slow however in the unlikely event that the \n",
    "# GCD is small (demonstrated below w GCD of 1 for last 2 examples)\n",
    "common_downsample = Resample(8000)\n",
    "slow_downsample = Resample(8001)\n",
    "slow_upsample = Resample(27101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "common_downsample(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "common_downsample(fake_multichannel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "slow_downsample(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "slow_upsample(audio_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Cropping/Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\"> CropSignal and CropTime can either be merged into one function, or they can outsource the bulk of their behavior to a shared cropping function</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mk_class('AudioPadType', **{o:o.lower() for o in ['Zeros', 'Zeros_After', 'Repeat']},\n",
    "         doc=\"All methods of padding audio as attributes to get tab-completion and typo-proofing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CropSignal(duration, pad_mode=AudioPadType.Zeros):\n",
    "    def _inner(ai: AudioItem)->AudioItem:\n",
    "        '''Crops signal to be length specified in ms by duration, padding if needed'''\n",
    "        sig = ai.sig.clone()\n",
    "        orig_samples = ai.nsamples\n",
    "        crop_samples = int((duration/1000)*ai.sr)\n",
    "        if orig_samples < crop_samples: \n",
    "            sig_pad = _tfm_pad_signal(sig, crop_samples, pad_mode=pad_mode)\n",
    "            return AudioItem((sig_pad, ai.sr, ai.path))\n",
    "        elif orig_samples == crop_samples: return AudioItem((sig, ai.sr, ai.path))\n",
    "        else:\n",
    "            crop_start = random.randint(0, int(orig_samples-crop_samples))\n",
    "            sig_crop = sig[:,crop_start:crop_start+crop_samples]\n",
    "            return AudioItem((sig_crop, ai.sr, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _tfm_pad_signal(sig, width, pad_mode=AudioPadType.Zeros):\n",
    "    '''Pad spectrogram to specified width, using specified pad mode'''\n",
    "    c,x = sig.shape\n",
    "    pad_m = pad_mode.lower()\n",
    "    if pad_m in [\"zeros\", \"zeros_after\"]:\n",
    "        zeros_front = random.randint(0, width-x) if pad_m == \"zeros\" else 0\n",
    "        pad_front = torch.zeros((c, zeros_front))\n",
    "        pad_back = torch.zeros((c, width-x-zeros_front))\n",
    "        return torch.cat((pad_front, sig, pad_back), 1)\n",
    "    elif pad_m == \"repeat\":\n",
    "        repeats = width//x + 1\n",
    "        return sig.repeat(1,repeats)[:,:width]\n",
    "    else:\n",
    "        raise ValueError(f\"pad_mode {pad_m} not currently supported, only 'zeros', 'zeros_after', or 'repeat'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropsig_1000ms = CropSignal(1000)\n",
    "cropsig_2000ms = CropSignal(2000)\n",
    "cropsig_5000ms = CropSignal(5000, pad_mode=AudioPadType.Zeros_After)\n",
    "print(f\"Audio is {audio_orig.duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud1s = cropsig_1000ms(audio_orig)\n",
    "aud2s = cropsig_2000ms(audio_orig)\n",
    "aud5s = cropsig_5000ms(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_orig.show()\n",
    "aud1s.show()\n",
    "aud2s.show()\n",
    "aud5s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(aud1s.nsamples, 1*audio_orig.sr)\n",
    "test_eq(aud2s.nsamples, 2*audio_orig.sr)\n",
    "test_eq(aud5s.nsamples, 5*audio_orig.sr)\n",
    "test_eq(aud1s.duration, 1)\n",
    "test_eq(aud2s.duration, 2)\n",
    "test_eq(aud5s.duration, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc1s = cropsig_1000ms(fake_multichannel)\n",
    "mc2s = cropsig_2000ms(fake_multichannel)\n",
    "mc5s = cropsig_5000ms(fake_multichannel)\n",
    "test_eq(mc1s.duration, 1)\n",
    "test_eq(mc2s.duration, 2)\n",
    "test_eq(mc5s.duration, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Signal Padding Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pad_mode zeros-after\n",
    "test_aud = AudioItem((torch.rand_like(audio_orig.sig), 16000, None))\n",
    "cropsig_pad = CropSignal(5000, pad_mode=AudioPadType.Zeros_After)\n",
    "z_after = cropsig_pad(test_aud)\n",
    "test_aud.hear()\n",
    "z_after.hear()\n",
    "# test end of signal is padded with zeros\n",
    "test_eq(z_after.sig[:,-10:], torch.zeros_like(z_after.sig)[:,-10:])\n",
    "# test front of signal is not padded with zeros\n",
    "test_ne(z_after.sig[:,0:10] , z_after.sig[:,-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pad_mode zeros by verifying signal begins and ends with zeros\n",
    "test_aud.hear()\n",
    "cropsig_pad = CropSignal(5000)\n",
    "z_after = cropsig_pad(test_aud)\n",
    "z_after.hear()\n",
    "test_eq(z_after.sig[:,0:2], z_after.sig[:,-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pad_mode repeat by making sure that columns are equal at the appropriate offsets\n",
    "cropsig_repeat = CropSignal(12000, pad_mode=AudioPadType.Repeat)\n",
    "ai_repeat = cropsig_repeat(audio_orig)\n",
    "ai_repeat.show()\n",
    "sig_repeat = ai_repeat.sig\n",
    "for i in range(audio_orig.nsamples):\n",
    "    test_eq(sig_repeat[:,i], sig_repeat[:,i+audio_orig.nsamples])\n",
    "    test_eq(sig_repeat[:,i], sig_repeat[:,i+2*audio_orig.nsamples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test bad pad_mode doesnt fail silently\n",
    "test_fail(CropSignal(12000, pad_mode=\"tenchify\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate repeat mode works on multichannel data (uncomment to see)\n",
    "mc_repeat = cropsig_repeat(fake_multichannel)\n",
    "#mc_repeat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping/Padding Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "aud1s = cropsig_1000ms(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "aud2s = cropsig_2000ms(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "aud5s = cropsig_5000ms(audio_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1 used scipy.ndimage.interpolation.shift but it was extremely slow (14-16ms) so I rewrote and got it down to 50Âµs\n",
    "def _shift(sig, s):\n",
    "    channels, samples = sig.shape[-2:]\n",
    "    if   s == 0: return torch.clone(sig)\n",
    "    elif  s < 0: return torch.cat([sig[...,-1*s:], torch.zeros_like(sig)[...,s:]], dim=-1)\n",
    "    else       : return torch.cat([torch.zeros_like(sig)[...,:s], sig[...,:samples-s]], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1,2,3,4,5,6,7,8,9,10]])\n",
    "t3 = torch.tensor([[1,2,3,4,5,6,7,8,9,10],[11,12,13,14,15,16,17,18,19,20],[21,22,23,24,25,26,27,28,29,30]])\n",
    "b4 = torch.stack([t3,t3,t3,t3])\n",
    "test_eq(b4.shape, torch.Size([4, 3, 10]))\n",
    "test_eq(_shift(t1,4), tensor([[0, 0, 0, 0, 1, 2, 3, 4, 5, 6]]))\n",
    "test_eq(_shift(t3,-2), tensor([[3,4,5,6,7,8,9,10,0,0],[13,14,15,16,17,18,19,20,0,0],[23,24,25,26,27,28,29,30,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ShiftSignal(max_pct=0.2, max_time=None, roll=False):\n",
    "    def _inner(ai: AudioItem)->AudioItem:\n",
    "        s = int(random.uniform(-1, 1)*max_pct*ai.nsamples if max_time is None else random.uniform(-1, 1)*max_time*ai.sr)\n",
    "        sig = torch.from_numpy(np.roll(ai.sig.numpy(), s, axis=1)) if roll else _shift(ai.sig, s) \n",
    "        return AudioItem((sig, ai.sr, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example without rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifter = ShiftSignal(max_pct=0.3)\n",
    "shifted = shifter(audio_orig)\n",
    "audio_orig.show()\n",
    "shifted.show()\n",
    "test_eq(audio_orig.sig.shape, shifted.sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a time shift of 1s never shifts more than 1s\n",
    "for i in range(100):\n",
    "    time_shifter = ShiftSignal(max_time=1)\n",
    "    just_ones = AudioItem((torch.ones(16000).unsqueeze(0), 16000, None))\n",
    "    shifted = time_shifter(just_ones)\n",
    "    test_eq(False, torch.allclose(shifted.sig, torch.zeros(16000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate shifting works on multichannel data (uncomment to see)\n",
    "shifter = ShiftSignal(max_time=2)\n",
    "mc_shifted = shifter(fake_multichannel)\n",
    "#mc_shifted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_and_roll = ShiftSignal(0.4, roll=True)\n",
    "shifted = shift_and_roll(audio_orig)\n",
    "audio_orig.show()\n",
    "shifted.show()\n",
    "test_eq(audio_orig.sig.shape, shifted.sig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shift Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "shifted = shifter(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "shifted = shift_and_roll(audio_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Noise to Signal\n",
    "\n",
    "Adds noise proportional to the energy of the signal (mean of abs value), and the specified noise level.\n",
    "\n",
    "This uses [colorednoise](https://github.com/felixpatzelt/colorednoise)(imported as 'cn'), developed by a data scientist named [Felix Patzelt](https://github.com/felixpatzelt). It allows you to use one simple function to create white, brown, pink and other [colors of noise](https://en.wikipedia.org/wiki/Colors_of_noise). Each color corresponds to an exponent, violet is -2, blue -1, white is 0, pink is 1, and brown is 2. We abstract this with a class that enumerates the list and shifts it down by two so the exponents are correct, and so that we get tab-completion.\n",
    "\n",
    "Because this actually draws a spectrogram and does an istft on it, it is about 10x faster if we implement our own white noise (simple and worth doing since it's the most common noise we'll want to use, this is what the `if color=0` line does, it overrides and generates white noise using our own simple algo.\n",
    "\n",
    "For just plain white noise, if we revert to remove the dependency on this library, the noise can be created with  \n",
    "`noise = torch.randn_like(ai.sig) * ai.sig.abs().mean() * noise_level`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "mk_class('NoiseColor', **{o:i-2 for i,o in enumerate(['Violet', 'Blue', 'White', 'Pink', 'Brown'])},\n",
    "         doc=\"All possible colors of noise as attributes to get tab-completion and typo-proofing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def AddNoise(noise_level=0.05, color=NoiseColor.White):\n",
    "    def _inner(ai: AudioItem)->AudioItem:\n",
    "        # if it's white noise, implement our own for speed\n",
    "        if color==0: noise = torch.randn_like(ai.sig)\n",
    "        else:        noise = torch.from_numpy(cn.powerlaw_psd_gaussian(exponent=color, size=ai.nsamples)).float()\n",
    "        scaled_noise = noise * ai.sig.abs().mean() * noise_level\n",
    "        return AudioItem((ai.sig + scaled_noise, ai.sr, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White noise examples (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = AddNoise()(audio_orig)\n",
    "real_noisy = AddNoise(noise_level=0.5)(audio_orig)\n",
    "msgs = [\"Original Audio\", \"5% White Noise\", \"50% White Noise\"]\n",
    "for i, aud in enumerate([audio_orig, noisy, real_noisy]):\n",
    "    print(msgs[i])\n",
    "    aud.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pink Noise Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = AddNoise(color=NoiseColor.Pink)(audio_orig)\n",
    "real_noisy = AddNoise(noise_level=1, color=NoiseColor.Pink)(audio_orig)\n",
    "msgs = [\"Original Audio\", \"5% Pink Noise\", \"100% Pink Noise\"]\n",
    "for i, aud in enumerate([audio_orig, noisy, real_noisy]):\n",
    "    print(msgs[i])\n",
    "    aud.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate blue-noise on multichannel data (uncomment to see)\n",
    "noisy = AddNoise(noise_level=0.5, color=NoiseColor.Blue)(fake_multichannel)\n",
    "#noisy.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "noise = torch.from_numpy(cn.powerlaw_psd_gaussian(exponent=0, size=audio_orig.nsamples)).float()\n",
    "scaled_noise = noise * audio_orig.sig.abs().mean() * 0.05\n",
    "out = AudioItem((audio_orig.sig + scaled_noise,audio_orig.sr, audio_orig.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "#Same speed for white noise and brown noise using their algorithm\n",
    "noise = torch.from_numpy(cn.powerlaw_psd_gaussian(exponent=2, size=audio_orig.nsamples)).float()\n",
    "scaled_noise = noise * audio_orig.sig.abs().mean() * 0.05\n",
    "out = AudioItem((audio_orig.sig + scaled_noise,audio_orig.sr, audio_orig.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "noisy = AddNoise()(audio_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-warning\"><strong>Note:</strong><br> \n",
    "   This will increase/decrease the energy of the signal but so far it appears to do nothing besides change the absolute values as the audios sound the same, and the spectrograms appear the same. The gain is being correctly applied, but the ipython audio player seems to normalize the volume level (confirmed by outputting and downloading the clips and confirming a difference in noise level). The spectrogram appears the same because it too does a form of normalization when it sets `ref`. We will likely need to adjust the ref value to something constant like np.max or 0 to stop this normalization, as the noise_level is often relevant for deep learning and not something we want to strip out.<br></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ChangeVolume(lower=0.1, upper=1.2):\n",
    "    def _inner(ai: AudioItem)->AudioItem:\n",
    "        gain = random.uniform(lower, upper)\n",
    "        return AudioItem((gain*ai.sig, ai.sr, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ipython player normalizes out volume difference, note different y-axis scale but same sound.\n",
    "volume_adjuster = ChangeVolume(lower=0.1, upper=0.2)\n",
    "altered = volume_adjuster(audio_orig)\n",
    "audio_orig.show()\n",
    "altered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust Volume Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "volume_adjuster(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "volume_adjuster(fake_multichannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SignalCutout(cut_pct=0.15):\n",
    "    def _inner(ai: AudioItem)->AudioItem:\n",
    "        copy = ai.sig.clone()\n",
    "        mask = torch.zeros(int(ai.nsamples*cut_pct))\n",
    "        mask_start = random.randint(0,ai.nsamples-len(mask))\n",
    "        copy[:,mask_start:mask_start+len(mask)] = mask\n",
    "        return AudioItem((copy, ai.sr, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = SignalCutout()\n",
    "cut = cutter(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate SignalCutout on multichannel, confirm the cuts align, uncomment to show\n",
    "cut_mc = SignalCutout(cut_pct=0.2)(fake_multichannel)\n",
    "#cut_mc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal Cutout Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "cutter(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "cutter(fake_multichannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Code adjusted from orig v1 fastai audio by Zack Caceres, Thom Mackey, and Stefano Giomo\n",
    "def SignalDrop(cut_pct=0.15):\n",
    "    def _inner(ai: AudioItem)->AudioItem:\n",
    "        \"\"\"Randomly replaces amplitude of signal with 0. Simulates analog info loss\"\"\"\n",
    "        copy = ai.sig.clone()\n",
    "        mask = (torch.rand_like(copy[0])>cut_pct).float()\n",
    "        masked = copy * mask\n",
    "        return AudioItem((masked, ai.sr, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropper = SignalDrop()\n",
    "dropped = dropper(audio_orig)\n",
    "dropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify SignalDrop is dropping both the correct number of samples, and dropping\n",
    "# the same samples from each channel, over a wide range of cut_pcts\n",
    "nsamples = fake_multichannel.nsamples\n",
    "for cut_pct in np.linspace(0.05, 0.5, 45):\n",
    "    dropped_mc = SignalDrop(cut_pct)(fake_multichannel)\n",
    "    match1 = (dropped_mc.sig[0] == dropped_mc.sig[1]).sum()\n",
    "    match2 = (dropped_mc.sig[0] == dropped_mc.sig[2]).sum()\n",
    "    match3 = (dropped_mc.sig[1] == dropped_mc.sig[2]).sum()\n",
    "    test_close(match1, cut_pct*nsamples, eps=.02*nsamples)\n",
    "    test_close(match2, cut_pct*nsamples, eps=.02*nsamples)\n",
    "    test_close(match3, cut_pct*nsamples, eps=.02*nsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal Drop Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "dropper(audio_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "dropper(fake_multichannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DownmixMono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# downmixMono was removed from torchaudio, we now just take the mean across channels\n",
    "# this works for both batches and individual items\n",
    "\n",
    "def DownmixMono():\n",
    "    def _inner(ai: AudioItem)->AudioItem:\n",
    "        \"\"\"Randomly replaces amplitude of signal with 0. Simulates analog info loss\"\"\"\n",
    "        downmixed = ai.sig.contiguous().mean(-2).unsqueeze(-2)\n",
    "        return AudioItem((downmixed, ai.sr, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downmixed = DownmixMono()(fake_multichannel)\n",
    "fake_multichannel.show()\n",
    "downmixed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test downmixing 1 channel has no effect\n",
    "downmixer = DownmixMono()\n",
    "downmixed = downmixer(audio_orig)\n",
    "test_eq(downmixed.sig, audio_orig.sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example showing a batch of 4 signals \n",
    "f2 = fake_multichannel.sig.unsqueeze(0)\n",
    "fake_batch = torch.cat([f2,f2,f2,f2], dim=0)\n",
    "downmixed = fake_batch.contiguous().mean(-2).unsqueeze(-2)\n",
    "print(\"Before shape:\", fake_batch.shape)\n",
    "print(\"After shape:\", downmixed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DownmixMono Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "downmixer(fake_multichannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Time Cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\"><strong>TO-DO:</strong><br> \n",
    "   1. In spectrogram when we pad with mean value we mess up normalization by altering std dev, how can we use fill values that dont mess things up<br></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CropTime(duration, pad_mode=AudioPadType.Zeros):\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        '''Random crops full spectrogram to be length specified in ms by crop_duration'''\n",
    "        sg = spectro.clone()\n",
    "        sr, hop = spectro.sr, spectro.hop_length\n",
    "        w_crop = int((sr*duration)/(1000*hop))+1\n",
    "        w_sg   = sg.shape[-1]\n",
    "        if   w_sg <  w_crop: \n",
    "            sg_pad = _tfm_pad_spectro(sg, w_crop, pad_mode=pad_mode)\n",
    "            return AudioSpectrogram.create(sg_pad, settings=spectro.settings)\n",
    "        elif w_sg == w_crop: return sg\n",
    "        else:\n",
    "            crop_start = random.randint(0, int(w_sg - w_crop))\n",
    "            sg_crop = sg[:,:,crop_start:crop_start+w_crop]\n",
    "            sg_crop.sample_start = int(crop_start*hop)\n",
    "            sg_crop.sample_end   = sg_crop.sample_start + int(duration*sr)\n",
    "            return AudioSpectrogram.create(sg_crop, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _tfm_pad_spectro(sg, width, pad_mode=AudioPadType.Zeros):\n",
    "    '''Pad spectrogram to specified width, using specified pad mode'''\n",
    "    c,y,x = sg.shape\n",
    "    pad_m = pad_mode.lower()\n",
    "    if pad_m in [\"zeros\", \"zeros_after\"]:\n",
    "        zeros_front = random.randint(0, width-x) if pad_m == \"zeros\" else 0\n",
    "        pad_front = torch.zeros((c,y, zeros_front))\n",
    "        pad_back = torch.zeros((c,y, width-x-zeros_front))\n",
    "        return AudioSpectrogram(torch.cat((pad_front, sg, pad_back), 2))\n",
    "    elif pad_m == \"repeat\":\n",
    "        repeats = width//x + 1\n",
    "        return sg.repeat(1,1,repeats)[:,:,:width]\n",
    "    else:\n",
    "        raise ValueError(f\"pad_mode {pad_m} not currently supported, only 'zeros', 'zeros_after', or 'repeat'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_1000ms = CropTime(1000)\n",
    "crop_2000ms = CropTime(2000)\n",
    "crop_5000ms = CropTime(5000)\n",
    "print(f\"Audio is {audio_orig.duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = crop_1000ms(sg_orig)\n",
    "s1.show()\n",
    "s2 = crop_2000ms(sg_orig)\n",
    "s2.show()\n",
    "s5 = crop_5000ms(sg_orig)\n",
    "s5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\"><strong>Note:</strong><br> \n",
    "    Because a spectrograms duration is dependent on rounding (samples/hop_length usually has a remainder that is padded up to an extra pixel), we cant use exact durations, so we must test_close instead of test_eq. This could be fixed by storing the AudioItems duration when the sg is generated, and also updating the duration manually anytime a Transform occurs that affects the size time axis (x-axis)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(sg_orig.settings, s1.settings)\n",
    "test_eq(sg_orig.settings, s5.settings)\n",
    "test_close(s1.width, int((1/sg_orig.duration)*sg_orig.width), eps=1.01)\n",
    "test_close(s2.width, int((2/sg_orig.duration)*sg_orig.width), eps=1.01)\n",
    "test_close(s5.width, int((5/sg_orig.duration)*sg_orig.width), eps=1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test AudioToSpec->CropTime and CropSignal->AudioToSpec will result in same size images\n",
    "oa = OpenAudio(files)\n",
    "crop_dur = random.randint(1000,5000)\n",
    "pipe_cropsig  = Pipeline([oa, AudioToSpec(hop_length=128), CropTime(crop_dur)], as_item=True)\n",
    "pipe_cropspec = Pipeline([oa, CropSignal(crop_dur), AudioToSpec(hop_length=128), ], as_item=True)\n",
    "for i in range(50):\n",
    "    test_eq(pipe_cropsig(i).width, pipe_cropspec(i).width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pad_mode zeros-after by verifying sg ends with zeros and begins with non-zeros\n",
    "crop_5000ms = CropTime(5000, pad_mode=AudioPadType.Zeros_After)\n",
    "s5 = crop_5000ms(sg_orig)\n",
    "test_eq(s5[:,:,-1], torch.zeros_like(s5)[:,:,-1])\n",
    "test_ne(s5[:,:,0], torch.zeros_like(s5)[:,:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_orig.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pad_mode repeat by making sure that columns are equal at the appropriate offsets\n",
    "crop_12000ms_repeat = CropTime(12000, pad_mode=AudioPadType.Repeat)\n",
    "s12_repeat = crop_12000ms_repeat(sg_orig)\n",
    "s12_repeat.show()\n",
    "for i in range(sg_orig.width):\n",
    "    test_eq(s12_repeat[:,:,i], s12_repeat[:,:,i+sg_orig.width])\n",
    "    test_eq(s12_repeat[:,:,i], s12_repeat[:,:,i+2*sg_orig.width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test bad pad_mode doesnt fail silently, correct is 'zeros_after'\n",
    "test_fail(CropTime(12000, pad_mode=\"zerosafter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.shape, s2.shape, s5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate on multichannel audio, uncomment to show\n",
    "sg_multi = a2s(fake_multichannel)\n",
    "s1_mc = crop_1000ms(sg_multi)\n",
    "#s1_mc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CropTime Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "#1s zero-padded crop\n",
    "crop_1000ms(sg_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "#5s zero-padded crop\n",
    "crop_5000ms(sg_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "#12s repeat-padded crop\n",
    "crop_12000ms_repeat(sg_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\"> If we wanted to we could make a class for these transforms that keeps the masked portion as state so that we could write a decodes method to go back to the original</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time and Frequency Masking (SpecAugment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MaskFreq(num_masks=1, size=20, start=None, val=None, **kwargs):\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        '''Google SpecAugment time masking from https://arxiv.org/abs/1904.08779.'''\n",
    "        nonlocal start\n",
    "        sg = spectro.clone()\n",
    "        channel_mean = sg.contiguous().view(sg.size(0), -1).mean(-1)[:,None,None]\n",
    "        mask_val = channel_mean if val is None else val\n",
    "        c, y, x = sg.shape\n",
    "        for _ in range(num_masks):\n",
    "            mask = torch.ones(size, x) * mask_val    \n",
    "            if start is None: start= random.randint(0, y-size)\n",
    "            if not 0 <= start <= y-size:\n",
    "                raise ValueError(f\"Start value '{start}' out of range for AudioSpectrogram of shape {sg.shape}\")\n",
    "            sg[:,start:start+size,:] = mask\n",
    "            start = None\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-warning\"> Passing around the settings manually is already fairly clunky, but is especially bad when we have to do it twice when MaskTime hands off to MaskFrequency. We should maybe make a copy of the AudioSpectrogram and then alter the tensor for it's sg rather than cloning out the sg and then building a new object at the end. Or just keep a reference to the parent tensor and pass that along, and have getattr recur looking for settings of the parents</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MaskTime(num_masks=1, size=20, start=None, val=None, **kwargs):\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        sg = spectro.clone()\n",
    "        sg = torch.einsum('...ij->...ji', sg)\n",
    "        sg = AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "        sg = MaskFreq(num_masks, size, start, val, **kwargs)(sg)\n",
    "        return AudioSpectrogram.create(torch.einsum('...ij->...ji', sg), settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_mask = MaskFreq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "freq_mask(sg_orig).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "time_mask = MaskTime()\n",
    "time_mask(sg_orig).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random frequency mask and test that it is being correctly applied\n",
    "size, start, val = [random.randint(1, 50) for i in range(3)]\n",
    "freq_mask_test = MaskFreq(size=size, start=start, val=val)\n",
    "sg_test = freq_mask_test(sg_orig)\n",
    "sg_test.show()\n",
    "test_eq(sg_test[:,start:start+size,:], val*torch.ones_like(sg_orig)[:,start:start+size,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random time mask and test that it is being correctly applied\n",
    "size, start, val = [random.randint(1, 50) for i in range(3)]\n",
    "time_mask_test = MaskTime(size=size, start=start, val=val)\n",
    "sg_test = time_mask_test(sg_orig)\n",
    "sg_test.show()\n",
    "test_eq(sg_test[:,:,start:start+size], val*torch.ones_like(sg_orig)[:,:,start:start+size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate on multichannel audio, uncomment to show, note bar is black so can be hard to see\n",
    "sg_multi = a2s(fake_multichannel)\n",
    "masked_mc = MaskFreq(size=40)(sg_multi)\n",
    "#masked_mc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpecAugment Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "freq_mask(sg_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "# time masking ~80Âµs slower because we transpose, delegate to MaskFreq, and transpose back, we could\n",
    "# fix this at the expense of a bit more code \n",
    "time_mask(sg_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "freq_mask(sg_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram Rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SGRoll(max_shift_pct=0.5, direction=0, **kwargs):\n",
    "    '''Shifts spectrogram along x-axis wrapping around to other side'''\n",
    "    if int(direction) not in [-1, 0, 1]: \n",
    "        raise ValueError(\"Direction must be -1(left) 0(bidirectional) or 1(right)\")\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        nonlocal direction\n",
    "        direction = random.choice([-1, 1]) if direction == 0 else direction\n",
    "        sg = spectro.clone()\n",
    "        w = sg.shape[-1]\n",
    "        roll_by = int(w*random.random()*max_shift_pct*direction)\n",
    "        sg = sg.roll(roll_by, dims=-1)\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roller = SGRoll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_orig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roller(sg_orig).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roller(sg_orig).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fails occasionally when by chance roll is 0, but i dont want to change to >= or <= because \n",
    "#it wont detect a broken roll! Could maybe scrap this test, it's overly complex\n",
    "def _first_non_zero_col(t):\n",
    "    for i in range(t.shape[2]):\n",
    "        if(t[0,0,i].item() == 1): return i\n",
    "roll_spec = a2s(audio_orig)\n",
    "mid = int((roll_spec.width/2))-5\n",
    "test_spec = torch.zeros_like(roll_spec)\n",
    "test_spec[:,:,mid:mid+10] = 1\n",
    "roll_spec.data = test_spec\n",
    "left_roller = SGRoll(max_shift_pct=0.4, direction=-1)\n",
    "left_spec = left_roller(roll_spec).data\n",
    "right_roller = SGRoll(max_shift_pct=0.4, direction=1)\n",
    "right_spec = right_roller(roll_spec).data\n",
    "ostart, lstart, rstart = map(_first_non_zero_col, (test_spec, left_spec, right_spec))\n",
    "test(lstart, ostart, operator.lt)\n",
    "test(rstart, ostart, operator.gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate rolling on multichannel audio, uncomment to show\n",
    "sg_multi = a2s(fake_multichannel)\n",
    "rolled_mc = roller(sg_multi)\n",
    "#rolled_mc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGRollTiming Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "roller(sg_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "roller(sg_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta/Accelerate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\"><strong>TO-DO:</strong> Test delta as part of a pipeline to make sure SpecAugment/roll/interpolate...etc are working on multichannel</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _torchdelta(mel:AudioSpectrogram, order=1, width=9):\n",
    "    '''Converts to numpy, takes delta and converts back to torch, needs torchification'''\n",
    "    if(mel.shape[1] < width): \n",
    "        raise ValueError(f'''Delta not possible with current settings, inputs must be wider than \n",
    "        {width} columns, try setting max_to_pad to a larger value to ensure a minimum width''')\n",
    "    return AudioSpectrogram(torch.from_numpy(librosa.feature.delta(mel.numpy(), order=order, width=width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Delta(width=9):\n",
    "    td = partial(_torchdelta, width=width)\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        new_channels = [torch.stack([c, td(c, order=1), td(c, order=2)]) for c in spectro]\n",
    "        return AudioSpectrogram.create(torch.cat(new_channels), settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = Delta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = delta(sg_orig)\n",
    "print(\"Shape\",d.shape)\n",
    "d.show()\n",
    "#nchannels for a spectrogram is how many channels its original audio had\n",
    "test_eq(d.nchannels, audio_orig.nchannels)\n",
    "test_eq(d.shape[1:], sg_orig.shape[1:])\n",
    "test_ne(d[0],d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate delta on multichannel audio, wont work until sg display is fixed\n",
    "sg_multi = a2s(fake_multichannel)\n",
    "delta_mc = delta(sg_multi)\n",
    "delta_mc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "delta(sg_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "delta(sg_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\"> This should probably be refactored to use visions size transform since it already exists</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def TfmResize(size, interp_mode=\"bilinear\", **kwargs):\n",
    "    '''Temporary fix to allow image resizing transform'''\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        nonlocal size\n",
    "        if isinstance(size, int): size = (size, size)\n",
    "        sg = spectro.clone()\n",
    "        c,y,x = sg.shape\n",
    "        sg = F.interpolate(sg.unsqueeze(0), size=size, mode=interp_mode, align_corners=False).squeeze(0)\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test when size is an int\n",
    "size=224\n",
    "resizer = TfmResize(size)\n",
    "resized = resizer(sg_orig)\n",
    "print(\"Original Shape: \", sg_orig.shape)\n",
    "print(\"Resized Shape :\" , resized.shape)\n",
    "test_eq(resized.shape[1:], torch.Size([size,size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test when size is a tuple with unequal values\n",
    "size_tup=(124,581)\n",
    "resizer_tup = TfmResize(size_tup)\n",
    "resized_tup = resizer_tup(sg_orig)\n",
    "print(\"Original Shape: \", sg_orig.shape)\n",
    "print(\"Resized Shape :\" , resized_tup.shape)\n",
    "resized_tup.show()\n",
    "test_eq(resized_tup.shape[1:], torch.Size(size_tup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate resizing on multichannel audio, uncomment to show\n",
    "sg_multi = a2s(fake_multichannel)\n",
    "resized_mc = TfmResize((200,100))(sg_multi)\n",
    "#resized_mc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resize Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "resizer(sg_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "resizer(sg_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = OpenAudio(files); oa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show simple preprocessing\n",
    "preprocess_pipe = Pipeline([oa, RemoveSilence(), CropSignal(2000), Resample(8000)], as_item=True)\n",
    "for i in range(3): preprocess_pipe(i).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show a very noisy set of signal augmentations\n",
    "augment_pipe1 = Pipeline([oa, RemoveSilence(), CropSignal(2000), AddNoise(noise_level=0.3), SignalDrop()], as_item=True)\n",
    "for i in range(3): augment_pipe1(i).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show another set of signal augmentations\n",
    "augment_pipe2 = Pipeline([oa, RemoveSilence(), CropSignal(2000), AddNoise(color=NoiseColor.Blue), \n",
    "                          ShiftSignal(roll=True), SignalCutout()], as_item=True)\n",
    "for i in range(3): augment_pipe2(i).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic melspectrogram pipe with advanced SpecAugment \n",
    "sg_cfg = AudioConfig.BasicMelSpectrogram(hop_length=256, n_fft=2048)\n",
    "pipe = Pipeline([oa, AudioToSpec.from_cfg(sg_cfg), CropTime(2000), MaskTime(num_masks=2, size=4), MaskFreq()], as_item=True)\n",
    "for i in range(5): pipe.show(pipe(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipe with only spectrogram transforms, notably Delta/Accelerate appended\n",
    "voice_cfg = AudioConfig.Voice()\n",
    "delta_pipe = Pipeline([oa, AudioToSpec.from_cfg(voice_cfg), CropTime(2000), Delta(), MaskTime(size=4), MaskFreq(), ], as_item=True)\n",
    "for i in range(5): delta_pipe.show(delta_pipe(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): pipe.show(pipe(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipe with signal and spectro transforms, and a lot of noise\n",
    "voice_cfg = AudioConfig.Voice()\n",
    "everything_pipe = Pipeline([oa, \n",
    "                            RemoveSilence(), CropSignal(2000), AddNoise(noise_level=0.3), SignalDrop(), \n",
    "                            AudioToSpec.from_cfg(voice_cfg), MaskTime(size=4), MaskFreq(), Delta()], as_item=True)\n",
    "for i in range(5): everything_pipe.show(everything_pipe(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
