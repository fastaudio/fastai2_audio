{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5cf645a372af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_basics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchaud_tfm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/condor/git/fastai2_audio_my/nbs/fastai2/torch_basics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch_imports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from fastai2.torch_basics import *\n",
    "from fastai2.data.all import *\n",
    "import torchaudio\n",
    "import torchaudio.transforms as torchaud_tfm\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "from dataclasses import dataclass, asdict, is_dataclass, make_dataclass\n",
    "from torchaudio.transforms import Spectrogram, AmplitudeToDB, MFCC\n",
    "from librosa.display import specshow, waveplot\n",
    "from inspect import signature\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "> Core functionality for the fastai audio library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioGetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section regroups the basic types used in vision with the transform that create objects of those types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "audio_extensions = tuple(str.lower(k) for k, v in mimetypes.types_map.items() if v.startswith('audio/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_audio_files(path, recurse=True, folders=None):\n",
    "    \"Get audio files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=audio_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def AudioGetter(suf='', recurse=True, folders=None):\n",
    "    \"Create `get_audio_files` partial function that searches path suffix `suf` and passes along `kwargs`, only in `folders`, if specified.\"\n",
    "    def _inner(o, recurse=recurse, folders=folders): \n",
    "        return get_audio_files(o/suf, recurse, folders)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "URLs.SPEAKERS10 = 'http://www.openslr.org/resources/45/ST-AEDS-20180100_1-OS.tgz'\n",
    "URLs.SPEAKERS250 = 'https://public-datasets.fra1.digitaloceanspaces.com/250-speakers.tar'\n",
    "URLs.ESC50 = 'https://github.com/karoldvl/ESC-50/archive/master.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tar_extract_at_filename(fname, dest):\n",
    "    \"Extract `fname` to `dest`/`fname`.name folder using `tarfile`\"\n",
    "    dest = Path(dest)/Path(fname).with_suffix('').name\n",
    "    tarfile.open(fname, 'r:gz').extractall(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = untar_data(URLs.SPEAKERS10, extract_func=tar_extract_at_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_getter = AudioGetter(\"\", recurse=True, folders=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = audio_getter(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files will load differently on different machines so we specify examples by name\n",
    "ex_files = [p/f for f in ['m0005_us_m0005_00218.wav', \n",
    "                                'f0003_us_f0003_00279.wav', \n",
    "                                'f0001_us_f0001_00168.wav', \n",
    "                                'f0005_us_f0005_00286.wav',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioTensor(TensorBase):\n",
    "    @classmethod\n",
    "    @delegates(torchaudio.load, keep=True)\n",
    "    def create(cls, fn, **kwargs):\n",
    "        sig, sr = torchaudio.load(fn, **kwargs)\n",
    "        return cls(sig, sr=sr)\n",
    "    \n",
    "    @property\n",
    "    def sr(self): return self.get_meta('sr')\n",
    "    \n",
    "    def __new__(cls, x, sr, **kwargs):\n",
    "        return super().__new__(cls, x, sr=sr, **kwargs)\n",
    "    \n",
    "    # This one should probably use set_meta() but there is no documentation,\n",
    "    # and I could not get it to work. Even TensorBase.set_meta?? is pointing\n",
    "    # to the wrong source because of fastai patch on Tensorbase to retain types\n",
    "    @sr.setter\n",
    "    def sr(self, val): self._meta['sr'] = val\n",
    "    \n",
    "    nsamples, nchannels = add_props(lambda i, self: self.shape[-1*(i+1)])\n",
    "    @property\n",
    "    def duration(self): return self.nsamples/float(self.sr)\n",
    "    \n",
    "    def hear(self):\n",
    "        display(Audio(self, rate=self.sr))\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show audio using `merge(self._show_args, kwargs)`\"\n",
    "        self.hear()\n",
    "        show_audio_signal(self, ctx=ctx, **kwargs)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch on indexing to retain the AudioTensor type, so when indexing it stays the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_f(fn):\n",
    "    def _f(self, *args, **kwargs):\n",
    "        cls = self.__class__\n",
    "        res = getattr(super(TensorBase, self), fn)(*args, **kwargs)\n",
    "        return retain_type(res, self)\n",
    "    return _f\n",
    "setattr(AudioTensor, '__getitem__', _get_f('__getitem__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioTensor(torch.ones(10), sr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_audio_signal(ai, ctx, **kwargs):\n",
    "    if(ai.nchannels > 1):\n",
    "        _,axs = plt.subplots(ai.nchannels, 1, figsize=(6,4*ai.nchannels))\n",
    "        for i,channel in enumerate(ai):\n",
    "            waveplot(channel.numpy(), ai.sr, ax=axs[i], **kwargs)\n",
    "    else:\n",
    "        axs = plt.subplots(ai.nchannels, 1)[1] if ctx is None else ctx \n",
    "        waveplot(ai.squeeze(0).numpy(), ai.sr, ax=axs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0 = AudioTensor.create(ex_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.sr, item0.nchannels, item0.nsamples, item0.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(type(item0.data), torch.Tensor)\n",
    "test_eq(item0.sr, 16000)\n",
    "test_eq(item0.nchannels, 1)\n",
    "test_eq(item0.nsamples, 58240)\n",
    "test_eq(item0.duration, 3.64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item1 = AudioTensor.create(files[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.show()\n",
    "item1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 3 equal length portions of 3 different signals so we can stack them\n",
    "#for a fake multichannel example\n",
    "ai0, ai1, ai2 = map(AudioTensor.create, ex_files[1:4]);\n",
    "min_samples = min(ai0.nsamples, ai1.nsamples, ai2.nsamples)\n",
    "s0, s1, s2 = map(lambda x: x[:,:min_samples], (ai0, ai1, ai2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst0 = AudioTensor(torch.ones(10), sr=120)\n",
    "tst1 = AudioTensor(torch.ones(10), sr=150)\n",
    "(tst0 + tst1).sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(s0.shape, s1.shape)\n",
    "test_eq(s1.shape, s2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_multichannel = AudioTensor(torch.stack((s0, s1, s2), dim=1).squeeze(0), sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(fake_multichannel.nchannels, 3)\n",
    "test_eq(fake_multichannel.nsamples, 53760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_multichannel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OpenAudio(Transform):\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "\n",
    "    def encodes(self, i):\n",
    "        o = self.items[i]\n",
    "        return AudioTensor.create(o)\n",
    "    \n",
    "    def decodes(self, i)->Path: \n",
    "        return self.items[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repr of Transform is:  \n",
    "classname: self.use_as_item {self.encodes} {self.decodes}  \n",
    "encodes and decodes are TypeDispatches whose reprs are str of dict where k/v pair is typename and function that handles that type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = OpenAudio(files); oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstrate functionality of OpenAudio.encodes, the rest of the nb will\n",
    "#use files that are opened by name for reproducibility/testing\n",
    "oa = OpenAudio(files)\n",
    "item100 = oa.encodes(100)\n",
    "item100.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test open audio on a random set of files\n",
    "for i in range(10):\n",
    "    idx = random.randint(0, len(files)-1)\n",
    "    test_eq_type(oa.encodes(idx), AudioTensor.create(files[idx]))\n",
    "    test_eq_type(oa.decodes(idx), files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa.encodes(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa.decodes(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions to wrap TorchAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_GenSpec    = torchaudio.transforms.Spectrogram\n",
    "_GenMelSpec = torchaudio.transforms.MelSpectrogram\n",
    "_GenMFCC    = torchaudio.transforms.MFCC\n",
    "_ToDB       = torchaudio.transforms.AmplitudeToDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add AudioBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def AudioBlock(cls=AudioTensor): return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'><strong>Note: </strong><br> Overriding getattr to store the settings isnt ideal, but if we dump them all in as attributes by doing `x.__dict__.update(settings)` we then can't easily pass settings when we do a transform and create a new AudioSpectrogram objct. Potential fixes are<br>\n",
    "1. Having both a settings dict and updating the dict with all its attributes (this feels dirty)<br>\n",
    "2. Finding a way to implement deepcopy for AudioSpectrogram so that we can clone it efficiently<br>\n",
    "3. Dumping the spectrogram settings and having a method that collects them so it can be passed to the constructor when we make a new AudioSpectrogram object in a transform<br>\n",
    "\n",
    "**Update: #2 is now a reasonable option because we mutate in place and dont need to pass settings forward**\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioSpectrogram Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioSpectrogram(TensorImageBase):\n",
    "    @classmethod\n",
    "    def create(cls, sg_tensor, settings=None):\n",
    "        audio_sg = cls(sg_tensor)\n",
    "        audio_sg._settings = settings\n",
    "        return audio_sg\n",
    "        \n",
    "    @property\n",
    "    def duration(self):\n",
    "        # spectrograms round up length to fill incomplete columns,\n",
    "        # so we subtract 0.5 to compensate, wont be exact\n",
    "        return (self.hop_length*(self.shape[-1]-0.5))/self.sr\n",
    "    \n",
    "    height, width = add_props(lambda i, self: self.shape[i+1], n=2)\n",
    "    #using the line below instead of above will fix show_batch but break multichannel/delta display\n",
    "    #nchannels, height, width = add_props(lambda i, self: self.shape[i], n=3)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        if name == \"settings\": return self._settings\n",
    "        if not name.startswith('_'): return self._settings[name]\n",
    "        raise AttributeError(f\"{self.__class__.__name__} object has no attribute {name}\")\n",
    "        \n",
    "    def show(self, ctx=None, ax=None, figsize=None, **kwargs):\n",
    "        show_spectrogram(self, ctx=ctx, ax=ax, figsize=figsize,**kwargs)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'><strong>TO-DO:</strong><br>\n",
    "    1. Get colorbar and axes working for multiplot display <br>\n",
    "    2. Have someone who knows matplotlib better cleanup/refactor<br>\n",
    "    3. Plotting the spectrogram forces it to a uniform size, we may want to display either the\n",
    "    shape of the image, or display it to scale with something like plt.figure(figsize=(sg.width/30, sg.height/30))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def show_spectrogram(sg, ax, ctx, figsize, **kwargs):\n",
    "    ax = ifnone(ax,ctx)\n",
    "    nchannels = sg.nchannels\n",
    "    r, c = nchannels, sg.data.shape[0]//nchannels\n",
    "    proper_kwargs = get_usable_kwargs(specshow, sg._settings, exclude=[\"ax\", \"kwargs\", \"data\",])\n",
    "    if (r == 1 and c == 1):\n",
    "        _show_spectrogram(sg, ax, proper_kwargs, **kwargs)\n",
    "        plt.title(\"Channel 0 Image 0: {} X {}px\".format(*sg.shape[-2:]))\n",
    "    else:\n",
    "        if figsize is None: figsize = (4*c, 3*r)\n",
    "        if ax is None: _,ax = plt.subplots(r, c, figsize=figsize)\n",
    "        for i, channel in enumerate(sg.data):\n",
    "            if r == 1:\n",
    "                cur_ax = ax[i%c]\n",
    "            elif c == 1:\n",
    "                cur_ax = ax[i%r]\n",
    "            else:\n",
    "                cur_ax = ax[i//c,i%c]\n",
    "            width,height = sg.shape[-2:]\n",
    "            cur_ax.set_title(f\"Channel {i//c} Image {i%c}: {width} X {height}px\")\n",
    "            z = specshow(channel.numpy(), ax=cur_ax, **sg._show_args, **proper_kwargs)\n",
    "            #plt.colorbar(z, ax=cur_ax)\n",
    "            #ax=plt.gca() #get the current axes\n",
    "            #PCM=ax.get_children()[2] #get the mappable, the 1st and the 2nd are the x and y axes\n",
    "            #plt.colorbar(PCM, ax=ax, format='%+2.0f dB') \n",
    "            \n",
    "def _show_spectrogram(sg, ax, proper_kwargs, **kwargs):\n",
    "    if \"mel\" not in sg._settings: y_axis = None\n",
    "    else:                        y_axis = \"mel\" if sg.mel else \"linear\"\n",
    "    proper_kwargs.update({\"x_axis\":\"time\", \"y_axis\":y_axis,})\n",
    "    _ = specshow(sg.data.squeeze(0).numpy(), **sg._show_args, **proper_kwargs)\n",
    "    fmt = '%+2.0f dB' if \"to_db\" in sg._settings and sg.to_db else '%+2.0f'\n",
    "    plt.colorbar(format=fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram Generation: AudioToSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioToSpec(Transform):\n",
    "    def __init__(self, pipe, settings):\n",
    "        self.pipe = pipe\n",
    "        self.settings = settings\n",
    "    \n",
    "    @classmethod\n",
    "    def from_cfg(cls, audio_cfg):\n",
    "        cfg = asdict(audio_cfg) if is_dataclass(audio_cfg) else dict(audio_cfg)\n",
    "        transformer = SpectrogramTransformer(mel=cfg.pop(\"mel\"), to_db=cfg.pop(\"to_db\"))\n",
    "        return transformer(**cfg)\n",
    "    \n",
    "    def encodes(self, audio:AudioTensor):\n",
    "        self.settings.update({'sr':audio.sr, 'nchannels':audio.nchannels})\n",
    "        return AudioSpectrogram.create(self.pipe(audio.data), settings=dict(self.settings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SpectrogramTransformer(mel=True, to_db=True):\n",
    "    sg_type = {\"mel\":mel, \"to_db\":to_db}\n",
    "    transforms = _get_transform_list(sg_type)\n",
    "    pipe_noargs = partial(fill_pipeline, sg_type=sg_type, transform_list=transforms)\n",
    "    pipe_noargs.__signature__ = _get_signature(transforms)\n",
    "    return pipe_noargs\n",
    "\n",
    "def _get_transform_list(sg_type):\n",
    "    '''Builds a list of higher-order transforms with no arguments'''\n",
    "    transforms = L()\n",
    "    if sg_type[\"mel\"]:   transforms += _GenMelSpec\n",
    "    else:                transforms += _GenSpec\n",
    "    if sg_type[\"to_db\"]: transforms += _ToDB\n",
    "    return transforms\n",
    "\n",
    "def fill_pipeline(transform_list, sg_type, **kwargs):\n",
    "    '''Adds correct args to each transform'''\n",
    "    kwargs = _override_bad_defaults(dict(kwargs))\n",
    "    function_list = L()\n",
    "    settings = {}\n",
    "    for f in transform_list:\n",
    "        usable_kwargs = get_usable_kwargs(f, kwargs)\n",
    "        function_list += f(**usable_kwargs)\n",
    "        settings.update(usable_kwargs)\n",
    "    warn_unused(kwargs, settings)\n",
    "    return AudioToSpec(Pipeline(function_list), settings={**sg_type, **settings})\n",
    "\n",
    "def _get_signature(transforms):\n",
    "    '''Looks at transform list and extracts all valid args for tab completion'''\n",
    "    delegations = [delegates(to=f, keep=True) for f in transforms]\n",
    "    out = lambda **kwargs: None\n",
    "    for d in delegations: out = d(out)\n",
    "    return signature(out)\n",
    "        \n",
    "def _override_bad_defaults(kwargs):\n",
    "    if \"n_fft\" not in kwargs or kwargs[\"n_fft\"] is None:            kwargs[\"n_fft\"] = 1024\n",
    "    if \"win_length\" not in kwargs or kwargs[\"win_length\"] is None:  kwargs[\"win_length\"] = kwargs[\"n_fft\"] \n",
    "    if \"hop_length\" not in kwargs or kwargs[\"hop_length\"] is None:  kwargs[\"hop_length\"] = int(kwargs[\"win_length\"]/2)\n",
    "    return kwargs\n",
    "\n",
    "def warn_unused(all_kwargs, used_kwargs):\n",
    "    unused_kwargs = set(all_kwargs.keys()) - set(used_kwargs.keys())\n",
    "    for kwarg in unused_kwargs:\n",
    "        warnings.warn(f\"{kwarg} is not a valid arg name and was not used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'><strong>Note: </strong><br>The <code>function_list += f(**usable_kwargs)</code> only works if all args are keyword arguments, doesnt work for unnamed args. Could add in a get usable args that checks if default is inspect._empty. This also needs more tests</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'><strong>Note: </strong><br> If a function (e.g. specshow) accepts kwargs, this wont pass extra arguments because specshow doesnt accept all kwargs, and will break if you pass in unexpected ones, but we have no way of knowing what functions they delegate to and pulling out the relevant kwargs, so if there is something we know it accepts as a kwarg like \"cmap\" we need to pass it in manually  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_usable_kwargs` takes a function and a dictionary of kwargs that may or may not be relevant to that function and returns a dictionary of all the default values to that function, updated with the kwargs that can be successfully applied. This is done because, first it allows us to combine multiple functions into a single AudioToSpec Transform but only pass the appropriate kwargs, secondly because it allows us to keep a dictionary of the settings used to create the Spectrogram which is sometimes used in it's display and cropping, and third because it allows us to warn the user when they are passing in improper or unused kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_usable_kwargs(func, kwargs, exclude=None):\n",
    "    exclude = ifnone(exclude, [])\n",
    "    defaults = {k:v.default for k, v in inspect.signature(func).parameters.items() if k not in exclude}\n",
    "    usable = {k:v for k,v in kwargs.items() if k in defaults}\n",
    "    return {**defaults, **usable}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: testing with a function that only takes ```a``` and ```b``` as kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kwargs(a:int=10, b:int=20): pass\n",
    "\n",
    "kwargs = {'a':1, 'b':2}\n",
    "extra_kwargs = {'z':0, 'a':1, 'b':2, 'c':3}\n",
    "test_eq(get_usable_kwargs(test_kwargs,       kwargs    ), kwargs)\n",
    "test_eq(get_usable_kwargs(test_kwargs, extra_kwargs, []), kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0 = AudioTensor.create(ex_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBMelSpec = SpectrogramTransformer(mel=True, to_db=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2s = DBMelSpec(n_fft=2048, hop_length=128, n_mels=64, baloney=\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<<<<<<< HEAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = a2s(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=======`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>>>>>> master`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sg with weird settings for testing\n",
    "item0 = AudioTensor.create(ex_files[0])\n",
    "item1 = AudioTensor.create(ex_files[1])\n",
    "a2s = DBMelSpec(f_max = 20000, n_mels=137)\n",
    "sg = a2s(item0)\n",
    "sg1 = a2s(item1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.show()\n",
    "sg1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_mc = a2s(fake_multichannel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_mc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg._settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.nchannels, sg.height, sg.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the explicit settings were properly stored in the spectrogram object and can be accessed as attributes\n",
    "test_eq(sg.f_max, 20000)\n",
    "test_eq(sg.hop_length, 512)\n",
    "test_eq(sg.sr, item100.sr)\n",
    "test_eq(sg.mel, True)\n",
    "test_eq(sg.to_db, True)\n",
    "test_eq(sg.nchannels, 1)\n",
    "test_eq(sg.height, 137)\n",
    "test_eq(sg.n_mels, sg.height)\n",
    "test_eq(sg.width, 114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = {k:v.default for k, v in inspect.signature(_GenMelSpec).parameters.items()}\n",
    "a2s = DBMelSpec(f_max =20000, hop_length=345)\n",
    "sg = a2s(item100)\n",
    "test_eq(sg.n_mels, defaults[\"n_mels\"])\n",
    "test_eq(sg.n_fft , 1024)\n",
    "test_eq(sg.shape[1], sg.n_mels)\n",
    "test_eq(sg.hop_length, 345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the spectrogram and audio have same duration, both are computed\n",
    "# on the fly as transforms can change their duration\n",
    "test_close(sg.duration, item100.duration, eps=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if spectrograms are right-side up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2s_5hz = DBMelSpec(\n",
    "    sample_rate=16000,\n",
    "    n_fft=1024,\n",
    "    win_length=1024,\n",
    "    hop_length=512,\n",
    "    f_min=0.0,\n",
    "    f_max=20000,\n",
    "    pad=0,\n",
    "    n_mels=137,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_5hz = torch.Tensor([0.5 * np.cos(2 * np.pi * 5 * np.arange(0, 1.0, 1.0/16000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_5hz = AudioTensor(sine_5hz, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_5hz = a2s_5hz(at_5hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_5hz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing to make sure the lowest bin of the spectrogram has the highest value/most energy\n",
    "max_row = sg_5hz.max(dim=1).indices.mode().values.item()\n",
    "assert max_row < 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test warnings for missing/extra arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_W=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test warning for unused argument 'power' for melspec\n",
    "#tests AudioToSpec and its from_cfg class method\n",
    "voice_mel_cfg = {'mel':True, 'to_db':True, 'n_fft':2560, 'f_max':22050., 'n_mels':128, 'hop_length':256, 'power':2}\n",
    "test_warns(lambda: AudioToSpec.from_cfg(voice_mel_cfg), show=SHOW_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_warns(lambda: DBMelSpec(power=2, n_fft=2560, f_max=22050, n_mels=128), show=SHOW_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for unused arguments 'f_max' and 'n_mels' for non-mel Spectrogram\n",
    "voice_mel_cfg = {'mel':False, 'to_db':True, 'f_max':22050., 'n_mels':128, 'n_fft':2560, 'hop_length':256, 'power':2}\n",
    "test_warns(lambda: AudioToSpec.from_cfg(voice_mel_cfg), show=SHOW_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test warning for unused argument 'top_db' when db conversion not done\n",
    "voice_mel_cfg = {'mel':True, 'to_db':False, 'top_db':20, 'n_fft':2560, 'f_max':22050., 'n_mels':128, 'hop_length':256}\n",
    "test_warns(lambda: AudioToSpec.from_cfg(voice_mel_cfg), show=SHOW_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test warning for invalid argument 'doesntexist'\n",
    "voice_mel_cfg = {'mel':True, 'to_db':True,'doesntexist':True, 'n_fft':2560, 'f_max':22050., 'n_mels':128, 'hop_length':256}\n",
    "test_warns(lambda: AudioToSpec.from_cfg(voice_mel_cfg), show=SHOW_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AudioToSpec Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_to_db_mel = SpectrogramTransformer()()\n",
    "a_to_nondb_mel = SpectrogramTransformer(to_db=False)()\n",
    "a_to_db_nonmel = SpectrogramTransformer(mel=False)()\n",
    "a_to_nondb_non_mel = SpectrogramTransformer(mel=False, to_db=False)()\n",
    "a_to_db_mel_hyperparams = SpectrogramTransformer()(n_fft=8192, hop_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "a_to_db_mel(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "a_to_nondb_mel(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "a_to_nondb_mel(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "a_to_db_nonmel(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "a_to_nondb_non_mel(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "# Time can blow up as a factor of n_fft and hop_length. n_fft is best kept to a power of two, hop_length\n",
    "# doesn't matter except smaller = more time because we have more chunks to perform STFTs on\n",
    "a_to_db_mel_hyperparams(item0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AudioToSpec Timing Tests as audio length scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def time_variable_length_audios(f, max_seconds=30, sr=16000, channels=1):\n",
    "    times = []\n",
    "    audios = [AudioTensor(torch.randn(channels, sr*i), sr) for i in range(1,max_seconds+1,2)]\n",
    "    for a in audios:\n",
    "        start = time.time()\n",
    "        out = f(a)\n",
    "        end = time.time()\n",
    "        times.append(round(1000*(end-start), 2))\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a2s = SpectrogramTransformer()()\n",
    "max_seconds = 180\n",
    "times_mono = time_variable_length_audios(f=a2s, max_seconds=max_seconds)\n",
    "times_stereo = time_variable_length_audios(f=a2s, max_seconds=max_seconds, channels=2)\n",
    "plt.plot(np.arange(0,max_seconds,2), times_mono, label=\"mono\")\n",
    "plt.plot(np.arange(0,max_seconds,2), times_stereo, label=\"stereo\")\n",
    "plt.legend(['mono','stereo'])\n",
    "plt.title(\"Time Taken by AudioToSpec\")\n",
    "plt.xlabel(\"Audio Duration in Seconds\")\n",
    "plt.ylabel(\"Processing Time in ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'><strong>Issue:</strong><br>\n",
    "    MFCC is based on a melspectrogram so it accepts a bunch of the same arguments, but instead of passing them in explicitly, they are passed as a dict to \"melkwargs\". As a result, in the current state the mfcc has no current info about the hop_length (determines the width) that it was generated with. One option is grabbing the defaults from _GenMelSpec inside AudioToMFCC and pass it into the sg_settings. OTOH this could be an argument for lumping everything into AudioToSpec, including MFCC, and then we'd have the same access to _GenMelSpec arguments for tab-completion. We could also make AudioToMFCC have a 2nd delegation to _GenMelSpec, and then parse the MelSpec arguments ourselves and bundle them into melkwargs before passing them to torchaudio. This would break our concept of wrapping the external functions in internal references like _GenMelSpec, because we'd no longer be agnostic to how theyre implemented. One last note is that melkwargs will not accept extra keywords, only the ones that torchaudio.transforms.MelSpectrogram expects. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(_GenMFCC.__init__)\n",
    "class AudioToMFCC(Transform):\n",
    "    def __init__(self,**kwargs):\n",
    "        func_args = get_usable_kwargs(_GenMFCC, kwargs, [])\n",
    "        self.transformer = _GenMFCC(**func_args)\n",
    "        self.settings = func_args\n",
    "        \n",
    "    @classmethod\n",
    "    def from_cfg(cls, audio_cfg):\n",
    "        cfg = asdict(audio_cfg) if is_dataclass(audio_cfg) else audio_cfg\n",
    "        return cls(**cfg)\n",
    "    \n",
    "    def encodes(self, x:AudioTensor):\n",
    "        sg_settings = {\"sr\":x.sr, 'nchannels':x.nchannels, **self.settings}\n",
    "        return AudioSpectrogram.create(self.transformer(x).detach(), settings=sg_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0 = AudioTensor.create(ex_files[0])\n",
    "a2mfcc = AudioToMFCC()\n",
    "mfcc = a2mfcc(item0)\n",
    "test_eq(mfcc.n_mfcc, mfcc.data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc._settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_mfcc specified should determine the height of the mfcc\n",
    "item1 = AudioTensor.create(ex_files[1])\n",
    "n_mfcc = 67\n",
    "a2mfcc67 = AudioToMFCC(n_mfcc=n_mfcc)\n",
    "mfcc67 = a2mfcc67(item1)\n",
    "test_eq(mfcc67.shape[1], n_mfcc)\n",
    "print(mfcc67.shape)\n",
    "mfcc67.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of passing in melkwargs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2mfcc_kwargs = AudioToMFCC(melkwargs={\"hop_length\":1024, \"n_fft\":1024})\n",
    "mfcc_kwargs = a2mfcc_kwargs(item1)\n",
    "mfcc_kwargs.show()\n",
    "# make sure a new hop_length changes the resulting width\n",
    "test_ne(mfcc_kwargs.width, mfcc.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCC Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a2mfcc = AudioToMFCC()\n",
    "max_seconds = 180\n",
    "times_mono = time_variable_length_audios(f=a2mfcc, max_seconds=max_seconds)\n",
    "times_stereo = time_variable_length_audios(f=a2mfcc, max_seconds=max_seconds, channels=2)\n",
    "plt.plot(np.arange(0,max_seconds,2), times_mono, label=\"mono\")\n",
    "plt.plot(np.arange(0,max_seconds,2), times_stereo, label=\"stereo\")\n",
    "plt.legend(['mono','stereo'])\n",
    "plt.title(\"Time Taken by AudioToMFCC\")\n",
    "plt.xlabel(\"Audio Duration in Seconds\")\n",
    "plt.ylabel(\"Processing Time in ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB MelSpectrogram Pipe (Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_cfg = {'n_fft':2560,'hop_length':64}\n",
    "oa = OpenAudio(files)\n",
    "a2s = DBMelSpec(**mel_cfg)\n",
    "db_mel_pipe = Pipeline([oa,a2s], as_item=True)\n",
    "for i in range(5):\n",
    "    print(\"Shape:\", db_mel_pipe(i).shape)\n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Spectrogram (non-mel, non-db) Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'mel':False, 'to_db':False, 'hop_length':128, 'n_fft':400}\n",
    "oa = OpenAudio(files)\n",
    "a2s = AudioToSpec.from_cfg(cfg)\n",
    "db_mel_pipe = Pipeline([oa, a2s], as_item=True)\n",
    "for i in range(3):\n",
    "    print(\"Shape:\", db_mel_pipe(i).shape)\n",
    "    db_mel_pipe.show(db_mel_pipe(i))\n",
    "    test_eq(db_mel_pipe(i).hop_length, cfg[\"hop_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBScale non-melspectrogram Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = OpenAudio(files)\n",
    "a2s = SpectrogramTransformer(mel=False)()\n",
    "db_mel_pipe = Pipeline([oa, a2s], as_item=True)\n",
    "for i in range(3): \n",
    "    print(\"Shape:\", db_mel_pipe(i).shape)\n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe using from_cfg (config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-mel db-scale spectrogram, warning is expected as f_max is an argument to melspectrograms\n",
    "cfg = {'mel':False, 'to_db':True, 'n_fft':260, 'f_max':22050., 'hop_length':128}\n",
    "oa = OpenAudio(files)\n",
    "a2s = AudioToSpec.from_cfg(cfg)\n",
    "db_mel_pipe = Pipeline([oa, a2s], as_item=True)\n",
    "for i in range(3): \n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_mfcc_pipe = Pipeline([oa, AudioToMFCC(n_mfcc=40),], as_item=True)\n",
    "for i in range(3): \n",
    "    db_mfcc_pipe.show(db_mfcc_pipe(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioConfig Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def config_from_func(func, name, **kwargs):\n",
    "    params = inspect.signature(func).parameters.items()\n",
    "    namespace = {k:v.default for k, v in params}\n",
    "    namespace.update(kwargs)\n",
    "    return make_dataclass(name, namespace.keys(), namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioConfig():\n",
    "    #default configurations from the wrapped function\n",
    "    #make sure to pass in mel=False as kwarg for non-mel spec, and to_db=False for non db spec\n",
    "    BasicSpectrogram    = config_from_func(_GenSpec, \"BasicSpectrogram\", mel=False, to_db=True)\n",
    "    BasicMelSpectrogram = config_from_func(_GenMelSpec, \"BasicMelSpectrogram\", mel=True, to_db=True)\n",
    "    BasicMFCC           = config_from_func(_GenMFCC, \"BasicMFCC \")\n",
    "    #special configs with domain-specific defaults\n",
    "\n",
    "    Voice = config_from_func(_GenMelSpec, \"Voice\", mel=\"True\", to_db=\"False\", f_min=50., f_max=8000., n_fft=1024, n_mels=128, hop_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<<<<<<< HEAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Mel Spectrogram is just the Torchaudio defaults, which are currently bad, hence\n",
    "# the empty melbins in the spectrogram below. We can make our own custom good ones like Voice\n",
    "mel_cfg = AudioConfig.BasicMelSpectrogram()\n",
    "a2mel = AudioToSpec.from_cfg(mel_cfg)\n",
    "item0 = AudioTensor.create(ex_files[0])\n",
    "mel_bad = a2mel(item0)\n",
    "mel_bad.show()\n",
    "voice_cfg = AudioConfig.Voice()\n",
    "a2mel = AudioToSpec.from_cfg(voice_cfg)\n",
    "mel_good = a2mel(oa(42))\n",
    "mel_good.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=======`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Mel Spectrogram is just the Torchaudio defaults, which are currently bad, hence\n",
    "# the empty melbins in the spectrogram below. We can make our own custom good ones like Voice\n",
    "mel_cfg = AudioConfig.BasicMelSpectrogram()\n",
    "a2mel = AudioToSpec.from_cfg(mel_cfg)\n",
    "item0 = AudioTensor.create(ex_files[0])\n",
    "mel_bad = a2mel(item0)\n",
    "mel_bad.show()\n",
    "voice_cfg = AudioConfig.Voice()\n",
    "a2mel = AudioToSpec.from_cfg(voice_cfg)\n",
    "mel_good = a2mel(oa(42))\n",
    "mel_good.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>>>>>> master`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mel_bad.n_fft, mel_cfg.n_fft)\n",
    "# hop defaults to None in torchaudio but is set later in the code, we override this default to None\n",
    "# internally in AudioToSpec to ensure the correct hop_length is stored as a sg attribute\n",
    "test_ne(mel_bad.hop_length, mel_cfg.hop_length)\n",
    "print(\"MelConfig Default Hop:\", mel_cfg.hop_length)\n",
    "print(\"Resulting Hop:\",mel_bad.hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_cfg = AudioConfig.BasicSpectrogram()\n",
    "# make sure mel setting is passed down and is false for normal spectro\n",
    "test_eq(sg_cfg.mel, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab a random file, test that the n_fft are passed successfully via config and stored in sg settings\n",
    "oa = OpenAudio(files)\n",
    "f_num = random.randint(0, len(files))\n",
    "sg_cfg = AudioConfig.BasicSpectrogram(n_fft=2000, hop_length=155)\n",
    "a2sg = AudioToSpec.from_cfg(sg_cfg)\n",
    "sg = a2sg(oa(f_num))\n",
    "test_eq(sg.n_fft, sg_cfg.n_fft)\n",
    "test_eq(sg.width, int(oa(f_num).nsamples/sg_cfg.hop_length)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline examples from Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = OpenAudio(files)\n",
    "db_mel_pipe = Pipeline([oa, AudioToSpec.from_cfg(sg_cfg)], as_item=True)\n",
    "for i in range(3): \n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_config = AudioConfig.Voice(); voice_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = OpenAudio(files)\n",
    "db_mel_pipe = Pipeline([oa, AudioToSpec.from_cfg(voice_config)], as_item=True)\n",
    "for i in range(3): \n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_cfg = AudioConfig.BasicMFCC()\n",
    "oa = OpenAudio(files)\n",
    "mfcc_pipe = Pipeline([oa, AudioToMFCC.from_cfg(mfcc_cfg)], as_item=True)\n",
    "for i in range(44,47):\n",
    "    print(\"Shape\", mfcc_pipe(i).shape)\n",
    "    mfcc_pipe(i).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
